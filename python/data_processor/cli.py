"""
Command line interface for programmatic access to the Data Processor.

The CLI intentionally focuses on two core workflows that users routinely
automate:

1. Quickly inspect a batch of files to discover available signals.
2. Run a lightweight processing pipeline (load → optional filtering →
   optional signal selection → export) defined either via CLI flags or a
   declarative JSON config file.

Example JSON pipeline:
{
  "files": ["./data/example.csv"],
  "combine": true,
  "selected_signals": ["time", "pressure", "temperature"],
  "filter": {
    "filter_type": "Moving Average",
    "ma_window": 5
  },
  "output": {
    "path": "./output/processed.csv",
    "format": "csv"
  }
}

Run with:
    python -m data_processor.cli run --config pipeline.json
"""

from __future__ import annotations

import json
from pathlib import Path

import pandas as pd
import typer
from rich.console import Console
from rich.table import Table

from .core.data_loader import DataLoader
from .core.signal_processor import SignalProcessor
from .models.processing_config import FilterConfig

console = Console()
app = typer.Typer(help="Data Processor CLI for automated workflows.")


def _load_config(config_path: Path) -> dict[str, object]:
    """Load a JSON pipeline configuration."""
    try:
        with config_path.open("r", encoding="utf-8") as fp:
            return json.load(fp)
    except json.JSONDecodeError as exc:
        raise typer.BadParameter(
            f"Invalid JSON in config '{config_path}': {exc}",
        ) from exc
    except OSError as exc:
        raise typer.BadParameter(
            f"Unable to read config '{config_path}': {exc}",
        ) from exc


def _select_signals(
    df: pd.DataFrame,
    selected_signals: list[str] | None,
    source_label: str,
) -> pd.DataFrame:
    """Return frame restricted to selected signals, warning about missing ones."""
    if not selected_signals:
        return df

    valid_signals = [col for col in selected_signals if col in df.columns]
    missing = sorted(set(selected_signals) - set(valid_signals))

    if missing:
        console.print(
            f"[yellow]Warning: missing signals skipped in {source_label} -> "
            f"{', '.join(missing)}[/yellow]",
        )

    if not valid_signals:
        raise typer.BadParameter(
            "None of the selected signals are present in the current dataset.",
        )

    return df[valid_signals]


def _apply_filter_if_requested(
    df: pd.DataFrame,
    filter_section: dict[str, object] | None,
    signal_processor: SignalProcessor,
) -> pd.DataFrame:
    """Apply configured filter if specified."""
    if not filter_section:
        return df

    filter_config = FilterConfig(**filter_section)
    return signal_processor.apply_filter(df, filter_config)


def _process_dataframe(
    df: pd.DataFrame,
    pipeline: dict[str, object],
    signal_processor: SignalProcessor,
    source_label: str,
) -> pd.DataFrame:
    """Run the configured operations for a single dataframe."""
    result = df.copy()
    result = _select_signals(
        result,
        pipeline.get("selected_signals"),
        source_label=source_label,
    )
    result = _apply_filter_if_requested(
        result,
        pipeline.get("filter"),
        signal_processor,
    )
    return result


def _format_output_filename(source_path: str, output_format: str) -> str:
    """Generate an output filename for per-file exports."""
    stem = Path(source_path).stem
    extension_map = {
        "csv": ".csv",
        "excel": ".xlsx",
        "xlsx": ".xlsx",
        "parquet": ".parquet",
        "json": ".json",
        "tsv": ".tsv",
    }
    suffix = extension_map.get(output_format.lower(), f".{output_format.lower()}")
    return f"{stem}{suffix}"


@app.command()
def detect(
    files: list[Path] = typer.Argument(..., help="One or more CSV/Parquet data files."),
    high_perf: bool = typer.Option(
        True,
        "--high-perf/--no-high-perf",
        help="Use the high performance loader.",
    ),
) -> None:
    """Detect and print unique signal names from the supplied files."""
    loader = DataLoader(use_high_performance=high_perf)
    file_paths = [str(path) for path in files]

    console.rule("Signal Detection")
    signals = loader.detect_signals(file_paths)
    if not signals:
        console.print("[yellow]No signals detected.[/yellow]")
        raise typer.Exit(code=0)

    table = Table(title="Detected Signals")
    table.add_column("Signal", justify="left")
    for signal in sorted(signals):
        table.add_row(signal)

    console.print(table)


@app.command()
def run(
    config: Path | None = typer.Option(
        None,
        "--config",
        "-c",
        help="Path to pipeline JSON config. CLI options override values inside.",
    ),
    files: list[Path] | None = typer.Option(
        None,
        "--file",
        "-f",
        help="Input files (ignored when provided via config). May be repeated.",
    ),
    output: Path | None = typer.Option(
        None,
        "--output",
        "-o",
        help="Destination file path when not specified in config.",
    ),
    output_format: str = typer.Option(
        "csv",
        "--format",
        "-t",
        help="Output format fallback (csv/excel/parquet/json).",
    ),
    combine: bool | None = typer.Option(
        None,
        "--combine/--no-combine",
        help="Override combine flag from config.",
    ),
    high_perf: bool = typer.Option(
        True,
        "--high-perf/--no-high-perf",
        help="Use the high performance loader.",
    ),
) -> None:
    """Execute a lightweight processing pipeline."""
    pipeline: dict[str, object] = {}
    if config:
        pipeline.update(_load_config(config))

    if files:
        pipeline["files"] = [str(path) for path in files]

    if combine is not None:
        pipeline["combine"] = combine

    if output:
        pipeline.setdefault("output", {})
        pipeline["output"]["path"] = str(output)
        pipeline["output"]["format"] = output_format

    file_list = pipeline.get("files", [])
    if not file_list:
        raise typer.BadParameter(
            "No input files provided. Use --file or supply a config.",
        )

    loader = DataLoader(use_high_performance=high_perf)
    processor = SignalProcessor()

    combine_frames = pipeline.get("combine", True)
    console.rule("Loading data")
    data = loader.load_multiple_files(file_list, combine=combine_frames)

    output_section = pipeline.get("output")
    if combine_frames:
        dataframe = _process_dataframe(
            data,
            pipeline,
            processor,
            source_label="combined dataset",
        )

        if output_section:
            output_path = Path(output_section["path"]).expanduser()
            target_format = output_section.get("format", output_format)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            loader.save_dataframe(
                dataframe,
                str(output_path),
                format_type=target_format,
            )
            console.print(f"[green]Saved processed data to {output_path}[/green]")
        else:
            console.print("[cyan]Pipeline completed (no output specified).[/cyan]")
        return

    # combine_frames == False => process each file independently
    processed_frames: dict[str, pd.DataFrame] = {}
    for source_path, frame in data.items():
        processed_frames[source_path] = _process_dataframe(
            frame,
            pipeline,
            processor,
            source_label=Path(source_path).name,
        )

    if output_section:
        output_path = Path(output_section["path"]).expanduser()
        target_format = output_section.get("format", output_format)
        if output_path.suffix:
            raise typer.BadParameter(
                "When combine is disabled, the output path must be a directory.",
            )
        output_path.mkdir(parents=True, exist_ok=True)

        for source_path, processed_df in processed_frames.items():
            destination = output_path / _format_output_filename(
                source_path,
                target_format,
            )
            loader.save_dataframe(
                processed_df,
                str(destination),
                format_type=target_format,
            )

        console.print(
            f"[green]Saved processed data for {len(processed_frames)} files "
            f"to {output_path}[/green]",
        )
    else:
        console.print(
            "[cyan]Processed files (no output directory provided): "
            f"{', '.join(Path(p).name for p in processed_frames)}[/cyan]",
        )


def main() -> None:
    """Entry point for `python -m data_processor.cli`."""
    try:
        from .logging_config import init_default_logging
    except ImportError:
        from logging_config import init_default_logging

    init_default_logging()
    app()


if __name__ == "__main__":
    main()
